Now, we'll write and train a perceptron in `Keras`
and track the results with `wandb`.

## The Perceptron

The perceptron model dates
[back to the 1950s](https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a),
when US Navy Psychologist Frank Rosenblatt
designed a pattern-recognition machine
based on abstract principles of computing with artificial neurons
described by Alan Turing, Warren McCulloch, and Walter Pitts.

A single perceptron unit, aka a "neuron",
first computes a weighted sum of its inputs.
The numbers in the weighted sum are called the _weights_.
Then it adds a number to that sum.
That number is called the _bias_.
Then, most perceptron units apply some function
to their outputs:
throwing out negative values,
applying a threshold,
or something like that.

The process of "training" a perceptron to perform a task
involves adjusting the weights and biases
so that the outputs are correct answers.
In our case, a correct answer occurs when the neuron
corresponding to a given digit
(the 0th neuron for 0, the 8th neuron for 8, etc.)
produces the largest weighted sum for the input.

## `Keras` and `wandb`

`Keras` is a high-level library, based on `TensorFlow`,
that allows us to describe neural networks,
including perceptrons,
and then train them to perform tasks.

`wandb` is a library for tracking, visualizing, and sharing
the training and evaluation process of machine learning algorithms,
especially neural networks.

## Training a Perceptron with `Keras` and `wandb`

Open `perceptron-start.py`.

This script describes a very simple perceptron.
The most important line is
`model.add(tf.keras.layers.Dense(num_classes))`.
This makes one neuron for each type of digit
(`Dense(num_classes)`)
and then incorporates it into a model
(`model.add`).

The line with `model.compile` is the second-most important.
It describes the training process for our learning machine
and sends it off to be compiled down to fast code.
Importantly, it describes the `loss`:
a function that is big when the outputs are far from the correct answer
and small when they are close.
We choose the `m`ean `s`quared `e`rror.

It also says that we'd like to track how well our model is doing
using the `accuracy`:
how often is the largest output coming from the neuron
for the actual class of the digit.

Finally, we begin the training process by calling `model.fit`.
This is the line inside which our script spends most of its time.

Execute this script by calling
`python perceptron-start.py`{{execute}}.

Inside the terminal, you'll see outputs generated by `Keras`.
One is a progress bar, which shows how many examples we've shown the network.
When it hits 60,000, we've shown the network all of the data,
and we say that one _epoch_ has passed.
`Keras` will stop training our model once enough epochs have passed.

How well is the algorithm doing? (Hint: look at the accuracy).
How well do you think you might do by guessing randomly?
What about by always guessing 0?
Do we want the loss to small or large? Is it getting larger or smaller as we train?

Once you've trained this perceptron and considered these questions,
move on to the next step.
